{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1d6fa0-cfdf-4ac5-adae-9f2cf54a391f",
   "metadata": {},
   "source": [
    "# RAGAS- Performance Evaluation of RAG Systems- Mistral\n",
    "\n",
    "## InsureLLM Company Question Answering CHATBOT\n",
    "This project builds a low cost, high accuracy question answering system designed for employees of InsureLLM, an Insurance Tech company. The chatbot acts as an expert knowledge worker, helping staff quickly find accurate answers to domain specific queries. To achieve reliability, the system leverages Retrieval-Augmented Generation (RAG), combining document retrieval with LLM reasoning. This ensures responses are context-grounded, relevant, and scalable for enterprise use.\n",
    "\n",
    "This Project integrates RAGAS metrics (faithfulness, relevancy, precision, recall, correctness) to automatically assess answer quality, and saves detailed results for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b09d690-390b-43dd-a19a-fb22d21095ca",
   "metadata": {},
   "source": [
    "### Importing the Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ebb61dc-da8e-4805-b8e7-71ca1b770c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness,\n",
    ")\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace, HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import os\n",
    "import glob\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "import backoff\n",
    "import time\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0a573-97a7-47b3-ab3d-4427373c3ff6",
   "metadata": {},
   "source": [
    "### Loading the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525a23be-5c3c-4cdf-a146-d6cbdd3d6fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"mistral:latest\"\n",
    "DB_NAME = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1da5bc6-d90b-42a3-a6a8-be2babd235c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068c8608-4ad2-4cb4-961f-8610e334331b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GoutamSahu\\AppData\\Local\\Temp\\ipykernel_26804\\193636757.py:1: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=MODEL, temperature=0.7)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(model=MODEL, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3219942-fb70-43d0-b164-7603489f852f",
   "metadata": {},
   "source": [
    "### Loading the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "386959ab-69f1-45d0-9acd-d0abbb2fe1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b9176-9a6f-42cf-a059-c8e3b6986c0d",
   "metadata": {},
   "source": [
    "### Creating Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc2492cd-5992-4509-b5e7-84bfe5047642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1088, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "951c14ef-ed52-4bbc-9d9b-45aea6d46cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e3f3d-2bc9-4be2-a125-afebafae679a",
   "metadata": {},
   "source": [
    "### Documents in Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b058b0cb-c977-4eea-8125-2caf67208744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found: contracts, employees, products, company\n"
     ]
    }
   ],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bec1bd-1ca6-446b-83fc-a8e5a1090dd3",
   "metadata": {},
   "source": [
    "### Initializing Chroma Vectorstore with HuggingFace Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3696d70a-1b98-467c-9c35-f64d50b7ee17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GoutamSahu\\AppData\\Local\\Temp\\ipykernel_26804\\2030954124.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 123 documents\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "if os.path.exists(DB_NAME):\n",
    "    Chroma(persist_directory=DB_NAME, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=DB_NAME)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48707b90-23de-4bb4-89cd-d0f07e2e3cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 384 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1fa6f-6105-4e0b-ab6b-e55efcee6c4e",
   "metadata": {},
   "source": [
    "### RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8992bc7-d7e4-4fe8-ad62-1f18a6abcb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GoutamSahu\\AppData\\Local\\Temp\\ipykernel_26804\\1263719082.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(model=MODEL, temperature=0.7)\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "retriever = vectorstore.as_retriever()\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55320241-fdc0-437f-9434-5a72629722df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Insurellm is an innovative insurance technology firm with over 200 employees across the US. Founded by Avery Lancaster in 2015, it offers four insurance software products - Carllm, Homellm, Rellm, and Marketllm - catering to various sectors such as auto, home, reinsurance, and connecting consumers with providers. Insurellm has more than 300 clients worldwide and provides technical support from 9 AM to 7 PM EST, Monday through Friday, with a commitment to respond to all queries within 24 business hours.\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "query = \"Can you describe Insurellm in a few sentences\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452bebae-8ecd-4be3-8879-d6ee2d2b8e59",
   "metadata": {},
   "source": [
    "## RAGAS Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79267ac6-2183-4d38-9944-3a4b0ec2eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAGAS_MODEL = \"sonar-reasoning-pro\"\n",
    "CACHE_FILE = \"generation_cache.json\"\n",
    "REQUEST_DELAY = 1.0  # seconds between calls\n",
    "TIMEOUT = 60  # seconds per request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbefd115-900f-4ce0-93ac-d31d9f2ee0b8",
   "metadata": {},
   "source": [
    "### RAGAS Based Evaluation of RAG Systems Using Perplexity Sonar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a15d0e40-0b67-4c8e-a304-c5290760a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_llm = ChatOpenAI(\n",
    "    api_key=os.getenv(\"PERPLEXITY_API_KEY\"),\n",
    "    base_url=\"https://api.perplexity.ai\",\n",
    "    model=\"sonar-reasoning-pro\",\n",
    "    timeout=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7fe2fd-56a2-42ef-9f52-80116e6860b1",
   "metadata": {},
   "source": [
    "### Creating a new evaluation chain with source documents returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fc0ab8a-76cc-48d3-9dd4-7c31deaa5639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eval_chain() -> ConversationalRetrievalChain:\n",
    "    return ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOllama(model=MODEL, temperature=0),\n",
    "        retriever=retriever,\n",
    "        memory=ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True,\n",
    "            output_key=\"answer\" \n",
    "        ),\n",
    "        return_source_documents=True,\n",
    "        output_key=\"answer\"       \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607dc8d0-2d3a-4806-bd12-d070e52f9e54",
   "metadata": {},
   "source": [
    "### Safe Chain Invocation with Exponential Backoff and Retry Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69f61011-8601-4976-8441-37ba8217e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    (TimeoutError, Exception),  # you can add RateLimitError if using OpenAI SDK\n",
    "    max_tries=5\n",
    ")\n",
    "def safe_invoke(chain, q):\n",
    "    return chain.invoke({\"question\": q})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5011e00-24a5-4937-b8c4-be0ab028c1e8",
   "metadata": {},
   "source": [
    "### Cache Management: Load and Save Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec656fdc-eee4-4560-842a-92098933fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cache():\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_cache(cache):\n",
    "    with open(CACHE_FILE, \"w\") as f:\n",
    "        json.dump(cache, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a85fe-34bb-43da-9ca8-f8df75832cd3",
   "metadata": {},
   "source": [
    "### Running RAG generation and collecting answers and contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bab9d024-22aa-472f-8eae-fc7c1d91a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generation(questions, ground_truth=None, delay=REQUEST_DELAY):\n",
    "    results = []\n",
    "    failures = 0\n",
    "    cache = load_cache()\n",
    "    chain = make_eval_chain()\n",
    "\n",
    "    for i, q in enumerate(tqdm(questions, desc=\"Generating answers\")):\n",
    "        if q in cache:\n",
    "            answer, contexts = cache[q][\"answer\"], cache[q][\"contexts\"]\n",
    "        else:\n",
    "            try:\n",
    "                output = safe_invoke(chain, q)\n",
    "                answer = output.get(\"answer\", \"\").strip()\n",
    "                docs = output.get(\"source_documents\", [])\n",
    "                contexts = list({doc.page_content.strip()[:2000] for doc in docs if doc.page_content.strip()})\n",
    "                cache[q] = {\"answer\": answer, \"contexts\": contexts}\n",
    "                save_cache(cache)\n",
    "            except Exception as e:\n",
    "                print(f\"Generation failed for Q{i}: {e}\")\n",
    "                answer, contexts = \"\", []\n",
    "                failures += 1\n",
    "\n",
    "            time.sleep(delay)  # throttle\n",
    "\n",
    "        row = {\"question\": q, \"answer\": answer, \"contexts\": contexts}\n",
    "        if ground_truth:\n",
    "            row[\"ground_truth\"] = ground_truth[i]\n",
    "        results.append(row)\n",
    "\n",
    "    print(f\"Evaluated {len(questions)} questions with {failures} failures.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6aa06b-0151-4526-bfb5-1dea6c4c9648",
   "metadata": {},
   "source": [
    "### Building RAGAS Compatible Dataset from QA Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd07536c-77d2-4c93-b853-0afb3a9f73d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ragas_dataset(rows):\n",
    "    keys = [\"question\", \"answer\", \"contexts\"]\n",
    "    if all(\"ground_truth\" in r for r in rows):\n",
    "        keys.append(\"ground_truth\")\n",
    "    return Dataset.from_dict({k: [r.get(k, \"\") for r in rows] for k in keys})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9abd01-d850-46e7-bc5a-0fdafc9c0da5",
   "metadata": {},
   "source": [
    "### Running RAGAS evaluation and returning metrics DataFrame and summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12eeafe5-d357-421e-866c-6e0383b19198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ragas(ds):\n",
    "    metrics = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "    if \"ground_truth\" in ds.column_names:\n",
    "        metrics.append(answer_correctness)\n",
    "    ragas_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    result = evaluate(ds, metrics=metrics, llm=perplexity_llm, embeddings=ragas_embeddings)\n",
    "    df = result.to_pandas()\n",
    "    summary = {metric: round(df[metric].mean(skipna=True), 3) for metric in df.columns if df[metric].dtype != \"O\"}\n",
    "    return df, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d16146-fae7-4b89-859a-023424ce034c",
   "metadata": {},
   "source": [
    "### Save evaluation results to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8128824-55e1-4495-b99d-59998812fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_outputs(df, summary, path=\"./eval\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    df.to_csv(f\"{path}/results_rows.csv\", index=False)\n",
    "    with open(f\"{path}/summary.json\", \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"Saved results to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc3cc447-1f3a-4252-8aaf-23216cf3319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is Insurellm?\",\n",
    "    \"Who is Avery Lancaster\"\n",
    "]\n",
    "\n",
    "ground_truth = [\n",
    "    \"Insurellm is a platform for insurance-related language model tasks.\",\n",
    "    \"Avery Lancaster is Co-Founder & Chief Executive Officer (CEO) of Insurellm\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7bcd26f-7d39-4837-a7b4-7faf23e44e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 2/2 [00:00<00:00, 12157.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 2 questions with 0 failures.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a959fb0ec3d74ee4b07889e04fc28164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>answer_correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Insurellm?</td>\n",
       "      <td>[## Support\\n1. **Technical Support**: Technic...</td>\n",
       "      <td>Insurellm is an insurance tech startup that wa...</td>\n",
       "      <td>Insurellm is a platform for insurance-related ...</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.963939</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.196197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is Avery Lancaster</td>\n",
       "      <td>[- **2022**: **Satisfactory**  \\n  Avery focus...</td>\n",
       "      <td>Avery Lancaster is the Co-Founder and Chief Ex...</td>\n",
       "      <td>Avery Lancaster is Co-Founder &amp; Chief Executiv...</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.885467</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.406071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_input                                 retrieved_contexts  \\\n",
       "0      What is Insurellm?  [## Support\\n1. **Technical Support**: Technic...   \n",
       "1  Who is Avery Lancaster  [- **2022**: **Satisfactory**  \\n  Avery focus...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Insurellm is an insurance tech startup that wa...   \n",
       "1  Avery Lancaster is the Co-Founder and Chief Ex...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  Insurellm is a platform for insurance-related ...      0.875000   \n",
       "1  Avery Lancaster is Co-Founder & Chief Executiv...      0.857143   \n",
       "\n",
       "   answer_relevancy  context_precision  context_recall  answer_correctness  \n",
       "0          0.963939           0.500000             0.0            0.196197  \n",
       "1          0.885467           0.333333             1.0            0.406071  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = run_generation(questions, ground_truth)\n",
    "ds = build_ragas_dataset(rows)\n",
    "df, summary = run_ragas(ds)\n",
    "\n",
    "print(\"Sample results:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce2031c4-615f-4a46-a8bd-97b80c9b2fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Macro metric means:\n",
      "faithfulness: 0.866\n",
      "answer_relevancy: 0.925\n",
      "context_precision: 0.417\n",
      "context_recall: 0.500\n",
      "answer_correctness: 0.301\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMacro metric means:\")\n",
    "for k, v in summary.items():\n",
    "    print(f\"{k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023c8e8-5521-428c-baab-9436089a7eed",
   "metadata": {},
   "source": [
    "# OBSERVATIONS\n",
    "## RAGAS Evaluation on Mistral Local Model\n",
    "\n",
    "## Query 1: \"What is Insurellm?\"\n",
    "Faithfulness: 0.875 → The answer is highly consistent with the retrieved information.\n",
    "\n",
    "Answer Relevancy: 0.964 → The response strongly addresses the user’s query.\n",
    "\n",
    "Context Precision: 0.50 → Half of the retrieved chunks were useful for answering.\n",
    "\n",
    "Context Recall: 0.0 → Not all relevant information was retrieved.\n",
    "\n",
    "Answer Correctness: 0.196 → The answer only partially matches the ground-truth reference.\n",
    "\n",
    "\n",
    "### Interpretation: \n",
    "Retrieval has improved (precision > 0), so the model had some useful grounding. However, recall is still zero, meaning important supporting info was missed. That’s why correctness is still low.\n",
    "\n",
    "## Query 2: \"Who is Avery Lancaster?\"\n",
    "Faithfulness: 0.857 → The response is strongly consistent with retrieved context.\n",
    "\n",
    "Answer Relevancy: 0.885 → The answer is clearly relevant.\n",
    "\n",
    "Context Precision: 0.333 → Some retrieved chunks were useful.\n",
    "\n",
    "Context Recall: 1.0 → The retriever successfully pulled all needed information.\n",
    "\n",
    "Answer Correctness: 0.406 → The answer is much closer to the ground truth.\n",
    "\n",
    "\n",
    "### Interpretation: \n",
    "This is a strong result. With perfect recall, the model had all the right info available, which boosted both faithfulness and correctness.\n",
    "\n",
    "## Overall Insight\n",
    "Retrieval quality is improving \n",
    "\n",
    "Consistency is still uneven → For definitional queries (“What is Insurellm?”), recall is missing. For entity-based queries (“Who is Avery Lancaster?”), recall is strong.\n",
    "\n",
    "Correctness is improving but still low → Even when retrieval works, answer correctness is only ~0.40. This suggests the LLM still isn’t fully grounding itself in the retrieved chunks.\n",
    "\n",
    "Key next step → Focus on retrieval recall for conceptual questions and on prompting/model instructions so the LLM relies strictly on retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244de573-1703-457e-8e81-2acf26225303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54d8c9-230f-42f0-99be-1482ee5cc2a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7b70e-bcda-4213-a380-7719496252ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65be7ed-8b3c-49fc-a353-5b84dcf40fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d162f12-149f-4550-a3bb-8d3c4195f32e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04608da3-28c0-4d15-b3ab-92d123a80520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3397424-a8ce-449c-bb3a-694b954056d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe193c-0b54-4ade-84eb-7c48363267dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea1f359-28cf-4198-a528-c1dacae49473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34f9d2-9357-40c1-a64a-800951593934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671d243-b3ed-4cc8-a9b8-c01f80639664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b2baf-8eb7-4204-ba23-7a80f9bef528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf88534-336b-4bc2-b764-2d89eb703a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36558260-b00f-4e1b-b66d-48d5e0f0b0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
