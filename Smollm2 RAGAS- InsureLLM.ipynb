{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1d6fa0-cfdf-4ac5-adae-9f2cf54a391f",
   "metadata": {},
   "source": [
    "# RAGAS- Performance Evaluation of RAG Systems- Smollm2\n",
    "\n",
    "## InsureLLM Company Question Answering CHATBOT\n",
    "This project builds a low cost, high accuracy question answering system designed for employees of InsureLLM, an Insurance Tech company. The chatbot acts as an expert knowledge worker, helping staff quickly find accurate answers to domain specific queries. To achieve reliability, the system leverages Retrieval-Augmented Generation (RAG), combining document retrieval with LLM reasoning. This ensures responses are context-grounded, relevant, and scalable for enterprise use.\n",
    "\n",
    "This Project integrates RAGAS metrics (faithfulness, relevancy, precision, recall, correctness) to automatically assess answer quality, and saves detailed results for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b09d690-390b-43dd-a19a-fb22d21095ca",
   "metadata": {},
   "source": [
    "### Importing the Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ebb61dc-da8e-4805-b8e7-71ca1b770c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness,\n",
    ")\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace, HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import os\n",
    "import glob\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "import backoff\n",
    "import time\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0a573-97a7-47b3-ab3d-4427373c3ff6",
   "metadata": {},
   "source": [
    "### Loading the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525a23be-5c3c-4cdf-a146-d6cbdd3d6fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"smollm2:latest\"\n",
    "DB_NAME = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1da5bc6-d90b-42a3-a6a8-be2babd235c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068c8608-4ad2-4cb4-961f-8610e334331b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GoutamSahu\\AppData\\Local\\Temp\\ipykernel_21296\\193636757.py:1: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=MODEL, temperature=0.7)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(model=MODEL, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3219942-fb70-43d0-b164-7603489f852f",
   "metadata": {},
   "source": [
    "### Loading the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "386959ab-69f1-45d0-9acd-d0abbb2fe1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b9176-9a6f-42cf-a059-c8e3b6986c0d",
   "metadata": {},
   "source": [
    "### Creating Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc2492cd-5992-4509-b5e7-84bfe5047642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1088, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "951c14ef-ed52-4bbc-9d9b-45aea6d46cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e3f3d-2bc9-4be2-a125-afebafae679a",
   "metadata": {},
   "source": [
    "### Documents in Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b058b0cb-c977-4eea-8125-2caf67208744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found: products, company, contracts, employees\n"
     ]
    }
   ],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bec1bd-1ca6-446b-83fc-a8e5a1090dd3",
   "metadata": {},
   "source": [
    "### Initializing Chroma Vectorstore with HuggingFace Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3696d70a-1b98-467c-9c35-f64d50b7ee17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GoutamSahu\\AppData\\Local\\Temp\\ipykernel_21296\\2030954124.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 123 documents\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "if os.path.exists(DB_NAME):\n",
    "    Chroma(persist_directory=DB_NAME, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=DB_NAME)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48707b90-23de-4bb4-89cd-d0f07e2e3cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 384 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1fa6f-6105-4e0b-ab6b-e55efcee6c4e",
   "metadata": {},
   "source": [
    "### RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8992bc7-d7e4-4fe8-ad62-1f18a6abcb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GoutamSahu\\AppData\\Local\\Temp\\ipykernel_21296\\1263719082.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(model=MODEL, temperature=0.7)\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "retriever = vectorstore.as_retriever()\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55320241-fdc0-437f-9434-5a72629722df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insurellm is an innovative insurance tech firm founded by Avery Lancaster in 2015, initially developing Markellm as a marketplace connecting consumers with insurance providers. The company rapidly expanded its offerings, creating four insurance software products - Carllm, Homellm, Rellm, and Marketllm - serving a wide range of clients worldwide through more than 300 contracts. Insurellm offers technical support via email and phone, promising to respond within 24 business hours or prioritize emergency queries during the contract period.\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "query = \"Can you describe Insurellm in a few sentences\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452bebae-8ecd-4be3-8879-d6ee2d2b8e59",
   "metadata": {},
   "source": [
    "## RAGAS Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79267ac6-2183-4d38-9944-3a4b0ec2eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAGAS_MODEL = \"sonar-reasoning-pro\"\n",
    "CACHE_FILE = \"generation_cache.json\"\n",
    "REQUEST_DELAY = 1.0  # seconds between calls\n",
    "TIMEOUT = 60  # seconds per request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbefd115-900f-4ce0-93ac-d31d9f2ee0b8",
   "metadata": {},
   "source": [
    "### RAGAS Based Evaluation of RAG Systems Using Perplexity Sonar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a15d0e40-0b67-4c8e-a304-c5290760a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_llm = ChatOpenAI(\n",
    "    api_key=os.getenv(\"PERPLEXITY_API_KEY\"),\n",
    "    base_url=\"https://api.perplexity.ai\",\n",
    "    model=\"sonar-reasoning-pro\",\n",
    "    timeout=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7fe2fd-56a2-42ef-9f52-80116e6860b1",
   "metadata": {},
   "source": [
    "### Creating a new evaluation chain with source documents returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fc0ab8a-76cc-48d3-9dd4-7c31deaa5639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eval_chain() -> ConversationalRetrievalChain:\n",
    "    return ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOllama(model=MODEL, temperature=0),\n",
    "        retriever=retriever,\n",
    "        memory=ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True,\n",
    "            output_key=\"answer\" \n",
    "        ),\n",
    "        return_source_documents=True,\n",
    "        output_key=\"answer\"       \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607dc8d0-2d3a-4806-bd12-d070e52f9e54",
   "metadata": {},
   "source": [
    "### Safe Chain Invocation with Exponential Backoff and Retry Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69f61011-8601-4976-8441-37ba8217e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    (TimeoutError, Exception),  # you can add RateLimitError if using OpenAI SDK\n",
    "    max_tries=5\n",
    ")\n",
    "def safe_invoke(chain, q):\n",
    "    return chain.invoke({\"question\": q})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5011e00-24a5-4937-b8c4-be0ab028c1e8",
   "metadata": {},
   "source": [
    "### Cache Management: Load and Save Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec656fdc-eee4-4560-842a-92098933fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cache():\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_cache(cache):\n",
    "    with open(CACHE_FILE, \"w\") as f:\n",
    "        json.dump(cache, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a85fe-34bb-43da-9ca8-f8df75832cd3",
   "metadata": {},
   "source": [
    "### Running RAG generation and collecting answers and contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bab9d024-22aa-472f-8eae-fc7c1d91a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generation(questions, ground_truth=None, delay=REQUEST_DELAY):\n",
    "    results = []\n",
    "    failures = 0\n",
    "    cache = load_cache()\n",
    "    chain = make_eval_chain()\n",
    "\n",
    "    for i, q in enumerate(tqdm(questions, desc=\"Generating answers\")):\n",
    "        if q in cache:\n",
    "            answer, contexts = cache[q][\"answer\"], cache[q][\"contexts\"]\n",
    "        else:\n",
    "            try:\n",
    "                output = safe_invoke(chain, q)\n",
    "                answer = output.get(\"answer\", \"\").strip()\n",
    "                docs = output.get(\"source_documents\", [])\n",
    "                contexts = list({doc.page_content.strip()[:2000] for doc in docs if doc.page_content.strip()})\n",
    "                cache[q] = {\"answer\": answer, \"contexts\": contexts}\n",
    "                save_cache(cache)\n",
    "            except Exception as e:\n",
    "                print(f\"Generation failed for Q{i}: {e}\")\n",
    "                answer, contexts = \"\", []\n",
    "                failures += 1\n",
    "\n",
    "            time.sleep(delay)  # throttle\n",
    "\n",
    "        row = {\"question\": q, \"answer\": answer, \"contexts\": contexts}\n",
    "        if ground_truth:\n",
    "            row[\"ground_truth\"] = ground_truth[i]\n",
    "        results.append(row)\n",
    "\n",
    "    print(f\"Evaluated {len(questions)} questions with {failures} failures.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6aa06b-0151-4526-bfb5-1dea6c4c9648",
   "metadata": {},
   "source": [
    "### Building RAGAS Compatible Dataset from QA Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd07536c-77d2-4c93-b853-0afb3a9f73d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ragas_dataset(rows):\n",
    "    keys = [\"question\", \"answer\", \"contexts\"]\n",
    "    if all(\"ground_truth\" in r for r in rows):\n",
    "        keys.append(\"ground_truth\")\n",
    "    return Dataset.from_dict({k: [r.get(k, \"\") for r in rows] for k in keys})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9abd01-d850-46e7-bc5a-0fdafc9c0da5",
   "metadata": {},
   "source": [
    "### Running RAGAS evaluation and returning metrics DataFrame and summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12eeafe5-d357-421e-866c-6e0383b19198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ragas(ds):\n",
    "    metrics = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "    if \"ground_truth\" in ds.column_names:\n",
    "        metrics.append(answer_correctness)\n",
    "    ragas_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    result = evaluate(ds, metrics=metrics, llm=perplexity_llm, embeddings=ragas_embeddings)\n",
    "    df = result.to_pandas()\n",
    "    summary = {metric: round(df[metric].mean(skipna=True), 3) for metric in df.columns if df[metric].dtype != \"O\"}\n",
    "    return df, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d16146-fae7-4b89-859a-023424ce034c",
   "metadata": {},
   "source": [
    "### Save evaluation results to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8128824-55e1-4495-b99d-59998812fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_outputs(df, summary, path=\"./eval\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    df.to_csv(f\"{path}/results_rows.csv\", index=False)\n",
    "    with open(f\"{path}/summary.json\", \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"Saved results to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc3cc447-1f3a-4252-8aaf-23216cf3319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is Insurellm?\",\n",
    "    \"Who is Avery Lancaster\"\n",
    "]\n",
    "\n",
    "ground_truth = [\n",
    "    \"Insurellm is a platform for insurance-related language model tasks.\",\n",
    "    \"Avery Lancaster is Co-Founder & Chief Executive Officer (CEO) of Insurellm\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7bcd26f-7d39-4837-a7b4-7faf23e44e7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 2/2 [00:00<00:00, 18078.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 2 questions with 0 failures.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17218e32c92f4ca5bc5883a550ea4fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>answer_correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Insurellm?</td>\n",
       "      <td>[## Support\\n1. **Technical Support**: Technic...</td>\n",
       "      <td>Insurellm is an insurance tech startup that wa...</td>\n",
       "      <td>Insurellm is a platform for insurance-related ...</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.196197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is Avery Lancaster</td>\n",
       "      <td>[- **2022**: **Satisfactory**  \\n  Avery focus...</td>\n",
       "      <td>Avery Lancaster is the Co-Founder and Chief Ex...</td>\n",
       "      <td>Avery Lancaster is Co-Founder &amp; Chief Executiv...</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.92493</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.406071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_input                                 retrieved_contexts  \\\n",
       "0      What is Insurellm?  [## Support\\n1. **Technical Support**: Technic...   \n",
       "1  Who is Avery Lancaster  [- **2022**: **Satisfactory**  \\n  Avery focus...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Insurellm is an insurance tech startup that wa...   \n",
       "1  Avery Lancaster is the Co-Founder and Chief Ex...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  Insurellm is a platform for insurance-related ...      0.875000   \n",
       "1  Avery Lancaster is Co-Founder & Chief Executiv...      0.714286   \n",
       "\n",
       "   answer_relevancy  context_precision  context_recall  answer_correctness  \n",
       "0           1.00000           0.000000             0.0            0.196197  \n",
       "1           0.92493           0.333333             1.0            0.406071  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = run_generation(questions, ground_truth)\n",
    "ds = build_ragas_dataset(rows)\n",
    "df, summary = run_ragas(ds)\n",
    "\n",
    "print(\"Sample results:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce2031c4-615f-4a46-a8bd-97b80c9b2fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Macro metric means:\n",
      "faithfulness: 0.795\n",
      "answer_relevancy: 0.962\n",
      "context_precision: 0.167\n",
      "context_recall: 0.500\n",
      "answer_correctness: 0.301\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMacro metric means:\")\n",
    "for k, v in summary.items():\n",
    "    print(f\"{k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023c8e8-5521-428c-baab-9436089a7eed",
   "metadata": {},
   "source": [
    "# OBSERVATIONS\n",
    "## RAGAS Evaluation on Smollm2 Local Model\n",
    "\n",
    "## Query 1: \"What is Insurellm?\"\n",
    "Faithfulness: 0.875 → The answer is highly consistent with retrieved context.\n",
    "\n",
    "Answer Relevancy: 1.0 → The response perfectly addresses the user’s question.\n",
    "\n",
    "Context Precision: 0.0 → None of the retrieved chunks were useful.\n",
    "\n",
    "Context Recall: 0.0 → No relevant information was retrieved.\n",
    "\n",
    "Answer Correctness: 0.196 → The answer only partially matched the ground-truth reference.\n",
    "\n",
    "\n",
    "### Interpretation: \n",
    "The model gave a relevant and faithful-sounding answer, but retrieval failed (precision = recall = 0). That’s why correctness stayed low — the model guessed from prior knowledge, not grounded context.\n",
    "\n",
    "## Query 2: \"Who is Avery Lancaster?\"\n",
    "Faithfulness: 0.714 → The answer is moderately consistent with the retrieved context.\n",
    "\n",
    "Answer Relevancy: 0.925 → The response is very relevant to the query.\n",
    "\n",
    "Context Precision: 0.333 → One-third of the retrieved chunks were useful.\n",
    "\n",
    "Context Recall: 1.0 → Retriever successfully retrieved all necessary information.\n",
    "\n",
    "Answer Correctness: 0.406 → The response aligns better with the ground-truth than Query 1, but still not perfect.\n",
    "\n",
    "\n",
    "### Interpretation: \n",
    "\n",
    "\n",
    "## Overall Insight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244de573-1703-457e-8e81-2acf26225303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
